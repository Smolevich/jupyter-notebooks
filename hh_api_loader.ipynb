{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:00:54.247035Z",
     "start_time": "2025-02-16T20:00:54.241125Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm \n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:00:54.770810Z",
     "start_time": "2025-02-16T20:00:54.767248Z"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG_PATH = os.path.expanduser('~/pet-projects/jupyter-notebooks/config.json')\n",
    "TOKEN_PATH = os.path.expanduser('~/pet-projects/jupyter-notebooks/token.json')\n",
    "BASE_DIR = os.path.expanduser('~/pet-projects/jupyter-notebooks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:00:55.301921Z",
     "start_time": "2025-02-16T20:00:55.284834Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_config():\n",
    "    \"\"\"Load configuration from config.json\"\"\"\n",
    "    try:\n",
    "        with open(CONFIG_PATH, 'r') as config_file:\n",
    "            return json.load(config_file)\n",
    "    except FileNotFoundError:\n",
    "        raise Exception(\"config.json file not found\")\n",
    "    except json.JSONDecodeError:\n",
    "        raise Exception(\"Error parsing config.json\")\n",
    "\n",
    "def save_token(token_data):\n",
    "    \"\"\"Save token to file\"\"\"\n",
    "    token_data['saved_at'] = datetime.now().isoformat()\n",
    "    with open(TOKEN_PATH, 'w') as token_file:\n",
    "        json.dump(token_data, token_file)\n",
    "\n",
    "def load_token():\n",
    "    \"\"\"Load existing token\"\"\"\n",
    "    try:\n",
    "        with open(TOKEN_PATH, 'r') as token_file:\n",
    "            token_data = json.load(token_file)\n",
    "            saved_at = datetime.fromisoformat(token_data['saved_at'])\n",
    "            # Check if token has expired (we store for 1 day)\n",
    "            if datetime.now() - saved_at < timedelta(days=1):\n",
    "                return token_data['access_token']\n",
    "    except (FileNotFoundError, json.JSONDecodeError, KeyError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def get_access_token(client_id, client_secret):\n",
    "    \"\"\"Get access token from HH.ru API\"\"\"\n",
    "    # First try to load existing token\n",
    "    existing_token = load_token()\n",
    "    if existing_token:\n",
    "        return existing_token\n",
    "\n",
    "    # If token not found or expired, request new one\n",
    "    token_url = 'https://hh.ru/oauth/token'\n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "    data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret\n",
    "    }\n",
    "    \n",
    "    response = requests.post(token_url, headers=headers, data=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        token_data = response.json()\n",
    "        save_token({\n",
    "            'access_token': token_data['access_token'],\n",
    "            'saved_at': datetime.now().isoformat()\n",
    "        })\n",
    "        return token_data['access_token']\n",
    "    else:\n",
    "        raise Exception(f\"Error getting token: {response.status_code}, {response.text}\")\n",
    "\n",
    "\n",
    "def create_api_client():\n",
    "    \"\"\"Create API client with loaded credentials\"\"\"\n",
    "    config = load_config()\n",
    "    \n",
    "    client_id = config.get('client_id')\n",
    "    client_secret = config.get('client_secret')\n",
    "    \n",
    "    if not client_id or not client_secret:\n",
    "        raise Exception(\"client_id or client_secret missing in config.json\")\n",
    "    \n",
    "    access_token = get_access_token(client_id, client_secret)\n",
    "    \n",
    "    return {\n",
    "        'headers': {\n",
    "            'Authorization': f'Bearer {access_token}',\n",
    "            'HH-User-Agent': 'Your_App_Name (your@email.com)'\n",
    "        },\n",
    "        'base_url': 'https://api.hh.ru'\n",
    "    }\n",
    "\n",
    "def get_vacancy_details(api_client, vacancy_id):\n",
    "    \"\"\"Get detailed information about a specific vacancy\"\"\"\n",
    "    response = requests.get(\n",
    "        f\"{api_client['base_url']}/vacancies/{vacancy_id}\",\n",
    "        headers=api_client['headers']\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error getting vacancy details: {response.status_code}, {response.text}\")\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "def get_all_vacancies(api_client, search_text, max_pages=None):\n",
    "    \"\"\"Get all vacancies with pagination\"\"\"\n",
    "    all_vacancies = []\n",
    "    page = 0\n",
    "    per_page = 100  # Maximum number of vacancies per page\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            f\"{api_client['base_url']}/vacancies\",\n",
    "            headers=api_client['headers'],\n",
    "            params = {\n",
    "                'text': search_text,\n",
    "                'per_page': per_page,\n",
    "                # 'area': 1,  # Moscow (can be removed or changed)\n",
    "                # 'only_with_salary': True,  # Optional: only with specified salary\n",
    "                'search_field': ['name'],  # Search in all fields\n",
    "                'order_by': 'publication_time',  # Sort by publication date\n",
    "                'page': page\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error getting page {page}: {response.status_code}\")\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        vacancies = data['items']\n",
    "        if not vacancies:\n",
    "            break\n",
    "            \n",
    "        # Save first 3 pages of raw JSON response\n",
    "        if page < 3:\n",
    "            filename = f\"vacancies_raw_page_{page}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            filepath = os.path.join(BASE_DIR, filename)\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Saved raw JSON for page {page} to {filename}\")\n",
    "            \n",
    "        if page == 0:\n",
    "            print(data)\n",
    "            \n",
    "        all_vacancies.extend(vacancies)\n",
    "        print(f\"Loaded page {page}, received {len(vacancies)} vacancies\")\n",
    "        \n",
    "        page += 1\n",
    "        if max_pages and page >= max_pages:\n",
    "            break\n",
    "    \n",
    "    return all_vacancies\n",
    "\n",
    "def create_vacancies_dataframe(vacancies):\n",
    "    \"\"\"Convert list of vacancies to DataFrame\"\"\"\n",
    "    processed_vacancies = []\n",
    "    \n",
    "    for vacancy in vacancies:\n",
    "        try:\n",
    "            # Safe extraction of nested dictionaries\n",
    "            salary_data = vacancy.get('salary') or {}\n",
    "            area_data = vacancy.get('area') or {}\n",
    "            snippet_data = vacancy.get('snippet') or {}\n",
    "            address_data = vacancy.get('address') or {}\n",
    "            empoyeer_data = vacancy.get('employer') or {}\n",
    "            experience_data = vacancy.get('experience') or {}\n",
    "            work_format_data = vacancy.get('work_format') or []\n",
    "            professional_roles = vacancy.get('professional_roles') or []\n",
    "            \n",
    "            \n",
    "            # Process work_format (can have multiple formats)\n",
    "            work_formats = [format.get('name', '') for format in work_format_data]\n",
    "            work_format_str = ', '.join(work_formats) if work_formats else ''\n",
    "            # Process professional_roles\n",
    "            role_ids = [str(role.get('id')) for role in professional_roles]\n",
    "            role_names = [role.get('name', '') for role in professional_roles]\n",
    "            \n",
    "            processed_vacancy = {\n",
    "                'id': vacancy.get('id'),\n",
    "                'name': vacancy.get('name'),\n",
    "                'url': vacancy.get('alternate_url'),\n",
    "                'salary_from': salary_data.get('from'),\n",
    "                'salary_to': salary_data.get('to'),\n",
    "                'salary_currency': salary_data.get('currency'),\n",
    "                'company_name': empoyeer_data.get('name'),\n",
    "                'company_id': empoyeer_data.get('id'),\n",
    "                'area': area_data.get('name'),\n",
    "                'address': address_data.get('raw', ''),\n",
    "                'created_at': vacancy.get('created_at'),\n",
    "                'requirement': snippet_data.get('requirement'),\n",
    "                'responsibility': snippet_data.get('responsibility'),\n",
    "                'experience': experience_data.get('name', ''),\n",
    "                'work_format': work_format_str,\n",
    "                'internship': vacancy.get('internship', False),\n",
    "                'premium': vacancy.get('premium', False),\n",
    "                'professional_role_ids': ','.join(role_ids),\n",
    "                'professional_role_names': ','.join(role_names)\n",
    "            \n",
    "            }\n",
    "            processed_vacancies.append(processed_vacancy)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing vacancy {vacancy.get('id', 'Unknown ID')}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not processed_vacancies:\n",
    "        raise Exception(\"Failed to process any vacancies\")\n",
    "        \n",
    "    df = pd.DataFrame(processed_vacancies)\n",
    "    \n",
    "    # Fill empty values\n",
    "    df['salary_from'] = df['salary_from'].fillna(0)\n",
    "    df['salary_to'] = df['salary_to'].fillna(0)\n",
    "    df['salary_currency'] = df['salary_currency'].fillna('RUR')\n",
    "    df['requirement'] = df['requirement'].fillna('')\n",
    "    df['responsibility'] = df['responsibility'].fillna('')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:01:03.697328Z",
     "start_time": "2025-02-16T20:00:55.900702Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    search_text = 'CPO'\n",
    "    # Пример запроса к API (получение вакансий)\n",
    "    api_client = create_api_client()\n",
    "    vacancies = get_all_vacancies(api_client, search_text, max_pages=5000)  # Ограничиваем 5 страницами для примера\n",
    "\n",
    "    # Преобразуем в DataFrame\n",
    "    df_part_one = create_vacancies_dataframe(vacancies)\n",
    "\n",
    "    search_text = 'Chief Product Owner'\n",
    "    vacancies = get_all_vacancies(api_client, search_text, max_pages=5000)  # Ограничиваем 5 страницами для примера\n",
    "\n",
    "    # Преобразуем в DataFrame\n",
    "    df_part_two = create_vacancies_dataframe(vacancies)\n",
    "    df = pd.concat([df_part_one, df_part_two])\n",
    "\n",
    "    print(f\"Всего получено вакансий: {len(df)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка: {str(e)}\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get details of a single vacancy with retry\n",
    "def get_vacancy_details_with_retry(api_client, vacancy_id, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return get_vacancy_details(api_client, vacancy_id)\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed to get details for vacancy {vacancy_id} after {max_retries} attempts: {e}\")\n",
    "                return None\n",
    "            time.sleep(1)  # Small delay before retry\n",
    "\n",
    "# Function for parallel processing of vacancies\n",
    "def process_vacancy_details(vacancy_id):\n",
    "    try:\n",
    "        details = get_vacancy_details_with_retry(api_client, vacancy_id)\n",
    "        if details:\n",
    "            return {\n",
    "                'id': vacancy_id,  # Add vacancy ID to the result\n",
    "                'description': details.get('description', ''),\n",
    "                'key_skills': [skill.get('name') for skill in details.get('key_skills', [])]\n",
    "            }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing vacancy {vacancy_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Getting detailed information about vacancies...\")\n",
    "\n",
    "# Get list of all vacancy IDs\n",
    "vacancy_ids = df['id'].tolist()\n",
    "total_vacancies = len(vacancy_ids)\n",
    "completed_vacancies = 0\n",
    "\n",
    "# Create progress bar\n",
    "progress_bar = tqdm(total=total_vacancies, desc=\"Processing vacancies\")\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Start getting details for all vacancies\n",
    "    future_to_id = {executor.submit(process_vacancy_details, vid): vid for vid in vacancy_ids}\n",
    "    \n",
    "    # Collect results\n",
    "    details_list = []\n",
    "    for future in as_completed(future_to_id):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            details_list.append(result)\n",
    "        # Update progress bar\n",
    "        completed_vacancies += 1\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# Close progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Create DataFrame with details\n",
    "details_df = pd.DataFrame(details_list)\n",
    "\n",
    "# Merge with main DataFrame using vacancy ID\n",
    "df = df.merge(details_df, on='id', how='left')\n",
    "df.info()\n",
    "# Fill empty values\n",
    "df['description'] = df['description'].fillna('')\n",
    "df['key_skills'] = df['key_skills'].fillna('').apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "print(f\"Details retrieved for {len(details_list)} out of {total_vacancies} vacancies\")\n",
    "print(\"\\nUpdated DataFrame structure:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_one_vacancy = False\n",
    "if debug_one_vacancy:   \n",
    "    id = 117803714\n",
    "    details = get_vacancy_details_with_retry(api_client, id)\n",
    "    filename = f\"vacancy_detail_{id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    filepath = os.path.join(BASE_DIR, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(details, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:01:14.196192Z",
     "start_time": "2025-02-16T20:01:14.179942Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:06:39.454366Z",
     "start_time": "2025-02-16T20:06:39.445619Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('company_name').size().reset_index(name='count').sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:06:43.174468Z",
     "start_time": "2025-02-16T20:06:43.145540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Формируем имя файла\n",
    "filename = f\"vacancies_{search_text.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "filepath = os.path.join(BASE_DIR, filename)\n",
    "\n",
    "# Сохраняем DataFrame\n",
    "df.to_csv(filepath, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of professional role distribution\n",
    "TOP_ROLES_COUNT = 15\n",
    "PLOT_HEIGHT = 600\n",
    "PLOT_WIDTH = 900\n",
    "FONT_SIZE = 12\n",
    "\n",
    "all_roles = []\n",
    "for roles in df['professional_role_names'].dropna():\n",
    "    if isinstance(roles, str) and roles:\n",
    "        all_roles.extend([role.strip() for role in roles.split(',')])\n",
    "\n",
    "roles_df = pd.DataFrame(all_roles, columns=['role']).value_counts().reset_index()\n",
    "roles_df.columns = ['role', 'count']\n",
    "roles_df = roles_df.sort_values('count', ascending=False)\n",
    "\n",
    "# Get top roles\n",
    "top_roles = roles_df.head(TOP_ROLES_COUNT)\n",
    "\n",
    "fig = px.bar(top_roles, \n",
    "             x='count', \n",
    "             y='role',\n",
    "             orientation='h',\n",
    "             title=f'Top {TOP_ROLES_COUNT} Professional Roles',\n",
    "             labels={'count': 'Number of Vacancies', 'role': 'Professional Role'},\n",
    "             color='count',\n",
    "             color_continuous_scale='Viridis',\n",
    "             text='count')\n",
    "\n",
    "fig.update_layout(\n",
    "    height=PLOT_HEIGHT,\n",
    "    width=PLOT_WIDTH,\n",
    "    yaxis={'categoryorder': 'total ascending'},\n",
    "    xaxis_title=\"Number of Vacancies\",\n",
    "    yaxis_title=\"Professional Role\",\n",
    "    font=dict(size=FONT_SIZE)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with vacancy count by city\n",
    "cities_df = df['area'].value_counts().reset_index()\n",
    "cities_df.columns = ['city', 'count']\n",
    "cities_df = cities_df.sort_values('count', ascending=False)\n",
    "\n",
    "# Limit number of cities to display (top-15)\n",
    "top_cities = cities_df.head(15).copy()\n",
    "\n",
    "# Create horizontal bar chart with value labels\n",
    "fig = px.bar(top_cities, \n",
    "             x='count', \n",
    "             y='city',\n",
    "             orientation='h',\n",
    "             title='Top 15 Cities by Number of Vacancies',\n",
    "             labels={'count': 'Number of Vacancies', 'city': 'City'},\n",
    "             color='count',\n",
    "             color_continuous_scale='Viridis',\n",
    "             text='count')  # Add text labels with values\n",
    "\n",
    "# Configure appearance and position of text labels\n",
    "fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "\n",
    "# Configure appearance\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=900,\n",
    "    yaxis={'categoryorder': 'total ascending'},\n",
    "    xaxis_title=\"Number of Vacancies\",\n",
    "    yaxis_title=\"City\",\n",
    "    font=dict(size=12),\n",
    "    uniformtext_minsize=8,  # Minimum font size for text\n",
    "    uniformtext_mode='hide'  # Hide text if it doesn't fit\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Additionally: percentage distribution of vacancies by city\n",
    "top_cities['percentage'] = (top_cities['count'] / top_cities['count'].sum() * 100).round(2)\n",
    "print(\"Percentage distribution of vacancies by city (top-15):\")\n",
    "print(top_cities[['city', 'count', 'percentage']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with vacancy count by company\n",
    "companies_df = df['company_name'].value_counts().reset_index()\n",
    "companies_df.columns = ['company', 'count']\n",
    "companies_df = companies_df.sort_values('count', ascending=False)\n",
    "\n",
    "# Limit number of companies to display (top-15)\n",
    "top_companies = companies_df.head(15).copy()\n",
    "\n",
    "# Create horizontal bar chart with value labels\n",
    "fig = px.bar(top_companies, \n",
    "             x='count', \n",
    "             y='company',\n",
    "             orientation='h',\n",
    "             title='Top 15 Companies by Number of Vacancies',\n",
    "             labels={'count': 'Number of Vacancies', 'company': 'Company'},\n",
    "             color='count',\n",
    "             color_continuous_scale='Viridis',\n",
    "             text='count')  # Add text labels with values\n",
    "\n",
    "# Configure appearance and position of text labels\n",
    "fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "\n",
    "# Configure appearance\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    yaxis={'categoryorder': 'total ascending'},\n",
    "    xaxis_title=\"Number of Vacancies\",\n",
    "    yaxis_title=\"Company\",\n",
    "    font=dict(size=12),\n",
    "    uniformtext_minsize=8,  # Minimum font size for text\n",
    "    uniformtext_mode='hide'  # Hide text if it doesn't fit\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Additionally: percentage distribution of vacancies by company\n",
    "top_companies['percentage'] = (top_companies['count'] / top_companies['count'].sum() * 100).round(2)\n",
    "print(\"Percentage distribution of vacancies by company (top-15):\")\n",
    "print(top_companies[['company', 'count', 'percentage']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe\n",
    "filtered_df = df.copy()\n",
    "\n",
    "# Convert created_at to datetime and calculate vacancy age in days\n",
    "filtered_df['created_at'] = pd.to_datetime(filtered_df['created_at'])\n",
    "filtered_df['vacancy_age_days'] = (pd.Timestamp.now(tz=filtered_df['created_at'].dt.tz) - filtered_df['created_at']).dt.days\n",
    "\n",
    "# Get role name for the title\n",
    "role_name = filtered_df['professional_role_names'].iloc[0]\n",
    "\n",
    "# Create a simple histogram\n",
    "fig = px.histogram(filtered_df,\n",
    "                  x='vacancy_age_days',\n",
    "                  nbins=20,\n",
    "                  title=f'Distribution of Vacancy Ages for {role_name}',\n",
    "                  labels={'vacancy_age_days': 'Vacancy Age (Days)',\n",
    "                         'count': 'Number of Vacancies'},\n",
    "                  color_discrete_sequence=['#636EFA'])\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    xaxis_title=\"Vacancy Age (Days)\",\n",
    "    yaxis_title=\"Number of Vacancies\",\n",
    "    bargap=0.1\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Print some basic statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Average vacancy age: {filtered_df['vacancy_age_days'].mean():.1f} days\")\n",
    "print(f\"Median vacancy age: {filtered_df['vacancy_age_days'].median():.1f} days\")\n",
    "print(f\"Newest vacancy: {filtered_df['vacancy_age_days'].min():.0f} days old\")\n",
    "print(f\"Oldest vacancy: {filtered_df['vacancy_age_days'].max():.0f} days old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode key_skills to get individual skills\n",
    "skills_df = df.explode('key_skills')\n",
    "\n",
    "# Count frequency of each skill\n",
    "skill_counts = skills_df['key_skills'].value_counts().reset_index()\n",
    "skill_counts.columns = ['skill', 'count']\n",
    "\n",
    "# Create bar chart of top 20 skills\n",
    "fig = px.bar(skill_counts.head(20), \n",
    "             x='skill',\n",
    "             y='count',\n",
    "             title='Top 20 Most Common Skills',\n",
    "             labels={'skill': 'Skill',\n",
    "                    'count': 'Number of Vacancies'},\n",
    "             color='count',\n",
    "             color_continuous_scale='Viridis')\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=45,\n",
    "    height=600,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate min and max salary for each currency, handling missing/zero values\n",
    "salary_range = df.groupby('salary_currency').agg({\n",
    "    'salary_from': lambda x: x[x > 0].min() if len(x[x > 0]) > 0 else 0,  # Min non-zero salary_from\n",
    "    'salary_to': lambda x: x[x > 0].max() if len(x[x > 0]) > 0 else 0     # Max non-zero salary_to\n",
    "}).reset_index()\n",
    "\n",
    "# Create a copy of the dataframe for plotting\n",
    "plot_data = []\n",
    "for _, row in salary_range.iterrows():\n",
    "    currency = row['salary_currency']\n",
    "    min_salary = row['salary_from'] if row['salary_from'] > 0 else row['salary_to']\n",
    "    max_salary = row['salary_to'] if row['salary_to'] > 0 else row['salary_from']\n",
    "    \n",
    "    # Add both min and max as separate rows for the same currency\n",
    "    if min_salary > 0:\n",
    "        plot_data.append({'salary_currency': currency, 'value': min_salary, 'type': 'Min'})\n",
    "    if max_salary > 0:\n",
    "        plot_data.append({'salary_currency': currency, 'value': max_salary, 'type': 'Max'})\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Create grouped bar chart showing min and max salaries\n",
    "fig = px.bar(plot_df, \n",
    "             x='salary_currency',\n",
    "             y='value',\n",
    "             color='type',\n",
    "             barmode='group',\n",
    "             title='Salary Range by Currency (Min and Max)',\n",
    "             labels={'salary_currency': 'Currency',\n",
    "                    'value': 'Salary Value',\n",
    "                    'type': 'Salary Type'},\n",
    "             text_auto='.0f',  # Add text labels with no decimal places\n",
    "             color_discrete_map={'Min': 'lightblue', 'Max': 'darkblue'})\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=500,\n",
    "    yaxis_type=\"log\",  # Use logarithmic scale for better visualization\n",
    ")\n",
    "\n",
    "# Update text position to be inside bars\n",
    "fig.update_traces(textposition='inside')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
