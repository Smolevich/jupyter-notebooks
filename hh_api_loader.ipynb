{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T08:10:22.517829Z",
     "start_time": "2025-04-15T08:10:22.302515Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm \n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from clickhouse_driver import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:00:54.770810Z",
     "start_time": "2025-02-16T20:00:54.767248Z"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG_PATH = os.path.expanduser('~/pet-projects/jupyter-notebooks/config.json')\n",
    "TOKEN_PATH = os.path.expanduser('~/pet-projects/jupyter-notebooks/token.json')\n",
    "BASE_DIR = os.path.expanduser('~/pet-projects/jupyter-notebooks/')\n",
    "DATA_DIR = os.path.expanduser('~/pet-projects/jupyter-notebooks/data/hh_api_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:00:55.301921Z",
     "start_time": "2025-02-16T20:00:55.284834Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_config():\n",
    "    \"\"\"Load configuration from config.json\"\"\"\n",
    "    try:\n",
    "        with open(CONFIG_PATH, 'r') as config_file:\n",
    "            return json.load(config_file)\n",
    "    except FileNotFoundError:\n",
    "        raise Exception(\"config.json file not found\")\n",
    "    except json.JSONDecodeError:\n",
    "        raise Exception(\"Error parsing config.json\")\n",
    "\n",
    "def save_token(token_data):\n",
    "    \"\"\"Save token to file\"\"\"\n",
    "    token_data['saved_at'] = datetime.now().isoformat()\n",
    "    with open(TOKEN_PATH, 'w') as token_file:\n",
    "        json.dump(token_data, token_file)\n",
    "\n",
    "def load_token():\n",
    "    \"\"\"Load existing token\"\"\"\n",
    "    try:\n",
    "        with open(TOKEN_PATH, 'r') as token_file:\n",
    "            token_data = json.load(token_file)\n",
    "            saved_at = datetime.fromisoformat(token_data['saved_at'])\n",
    "            # Check if token has expired (we store for 1 day)\n",
    "            if datetime.now() - saved_at < timedelta(days=1):\n",
    "                return token_data['access_token']\n",
    "    except (FileNotFoundError, json.JSONDecodeError, KeyError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def get_access_token(client_id, client_secret):\n",
    "    \"\"\"Get access token from HH.ru API\"\"\"\n",
    "    # First try to load existing token\n",
    "    existing_token = load_token()\n",
    "    if existing_token:\n",
    "        return existing_token\n",
    "\n",
    "    # If token not found or expired, request new one\n",
    "    token_url = 'https://hh.ru/oauth/token'\n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "    data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret\n",
    "    }\n",
    "    \n",
    "    response = requests.post(token_url, headers=headers, data=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        token_data = response.json()\n",
    "        save_token({\n",
    "            'access_token': token_data['access_token'],\n",
    "            'saved_at': datetime.now().isoformat()\n",
    "        })\n",
    "        return token_data['access_token']\n",
    "    else:\n",
    "        raise Exception(f\"Error getting token: {response.status_code}, {response.text}\")\n",
    "\n",
    "\n",
    "def create_api_client():\n",
    "    \"\"\"Create API client with loaded credentials\"\"\"\n",
    "    config = load_config()\n",
    "    \n",
    "    client_id = config.get('client_id')\n",
    "    client_secret = config.get('client_secret')\n",
    "    user_email = config.get('user_email')\n",
    "    \n",
    "    if not client_id or not client_secret:\n",
    "        raise Exception(\"client_id or client_secret missing in config.json\")\n",
    "    \n",
    "    access_token = get_access_token(client_id, client_secret)\n",
    "    \n",
    "    return {\n",
    "        'headers': {\n",
    "            'Authorization': f'Bearer {access_token}',\n",
    "            'HH-User-Agent': f'Your_App_Name ({user_email})'\n",
    "        },\n",
    "        'base_url': 'https://api.hh.ru'\n",
    "    }\n",
    "\n",
    "def get_vacancy_details(api_client, vacancy_id):\n",
    "    \"\"\"Get detailed information about a specific vacancy\"\"\"\n",
    "    response = requests.get(\n",
    "        f\"{api_client['base_url']}/vacancies/{vacancy_id}\",\n",
    "        headers=api_client['headers']\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error getting vacancy details: {response.status_code}, {response.text}\")\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "def get_employer_details(api_client, employer_id):\n",
    "    \"\"\"Get detailed information about a specific employer\"\"\"\n",
    "    response = requests.get(\n",
    "        f\"{api_client['base_url']}/employers/{employer_id}\",\n",
    "        headers=api_client['headers']\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error getting employer details: {response.status_code}, {response.text}\")\n",
    "        \n",
    "    return response.json()\n",
    "\n",
    "def get_all_vacancies_single(api_client, search_text, max_pages=None):\n",
    "    \"\"\"Get all vacancies with pagination\"\"\"\n",
    "    all_vacancies = []\n",
    "    page = 0\n",
    "    per_page = 100  # Maximum number of vacancies per page\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(\n",
    "            f\"{api_client['base_url']}/vacancies\",\n",
    "            headers=api_client['headers'],\n",
    "            params = {\n",
    "                'text': search_text,\n",
    "                'per_page': per_page,\n",
    "                # 'area': 1,  # Moscow (can be removed or changed)\n",
    "                # 'only_with_salary': True,  # Optional: only with specified salary\n",
    "                'search_field': ['name'],  # Search in all fields\n",
    "                'order_by': 'publication_time',  # Sort by publication date\n",
    "                'page': page\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error getting page {page}: {response.status_code}\")\n",
    "            break\n",
    "            \n",
    "        data = response.json()\n",
    "        vacancies = data['items']\n",
    "        \n",
    "        for vacancy in vacancies:\n",
    "            vacancy['search_text'] = search_text\n",
    "        \n",
    "        if not vacancies:\n",
    "            break\n",
    "            \n",
    "        # Save first 3 pages of raw JSON response\n",
    "        if page < 3:\n",
    "            filename = f\"vacancies_raw_page_{page}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            if not os.path.exists(DATA_DIR):\n",
    "                os.makedirs(DATA_DIR)\n",
    "            filepath = os.path.join(DATA_DIR, filename)\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Saved raw JSON for page {page} to {filename}\")\n",
    "            \n",
    "        if page == 0:\n",
    "            print(data)\n",
    "            \n",
    "        all_vacancies.extend(vacancies)\n",
    "        print(f\"Loaded page {page}, received {len(vacancies)} vacancies\")\n",
    "        \n",
    "        page += 1\n",
    "        if max_pages and page >= max_pages:\n",
    "            break\n",
    "    \n",
    "    return all_vacancies\n",
    "\n",
    "def get_all_vacancies(api_client, search_text, max_pages=None):\n",
    "    \"\"\"\n",
    "    Gets vacancies with pagination.\n",
    "    If the search_text parameter contains multiple values separated by '|',\n",
    "    sequential search is performed for each keyword.\n",
    "    \"\"\"\n",
    "    if '|' in search_text:\n",
    "        all_vacancies = []\n",
    "        keywords = [kw.strip() for kw in search_text.split('|')]\n",
    "        for kw in keywords:\n",
    "            print(f\"Starting search for: {kw}\")\n",
    "            vacancies = get_all_vacancies_single(api_client, kw, max_pages)\n",
    "            all_vacancies.extend(vacancies)\n",
    "        return all_vacancies\n",
    "    else:\n",
    "        return get_all_vacancies_single(api_client, search_text, max_pages)\n",
    "\n",
    "def create_vacancies_dataframe(vacancies):\n",
    "    \"\"\"Convert list of vacancies to DataFrame\"\"\"\n",
    "    processed_vacancies = []\n",
    "    \n",
    "    for vacancy in vacancies:\n",
    "        try:\n",
    "            # Safe extraction of nested dictionaries\n",
    "            salary_data = vacancy.get('salary') or {}\n",
    "            area_data = vacancy.get('area') or {}\n",
    "            snippet_data = vacancy.get('snippet') or {}\n",
    "            address_data = vacancy.get('address') or {}\n",
    "            empoyeer_data = vacancy.get('employer') or {}\n",
    "            experience_data = vacancy.get('experience') or {}\n",
    "            work_format_data = vacancy.get('work_format') or []\n",
    "            professional_roles = vacancy.get('professional_roles') or []\n",
    "            \n",
    "            \n",
    "            # Process work_format (can have multiple formats)\n",
    "            work_formats = [format.get('name', '') for format in work_format_data]\n",
    "            work_format_str = ', '.join(work_formats) if work_formats else ''\n",
    "            # Process professional_roles\n",
    "            role_ids = [str(role.get('id')) for role in professional_roles]\n",
    "            role_names = [role.get('name', '') for role in professional_roles]\n",
    "            \n",
    "            processed_vacancy = {\n",
    "                'id': vacancy.get('id'),\n",
    "                'name': vacancy.get('name'),\n",
    "                'url': vacancy.get('alternate_url'),\n",
    "                'salary_from': salary_data.get('from'),\n",
    "                'salary_to': salary_data.get('to'),\n",
    "                'salary_currency': salary_data.get('currency'),\n",
    "                'company_name': empoyeer_data.get('name'),\n",
    "                'company_id': empoyeer_data.get('id'),\n",
    "                'area': area_data.get('name'),\n",
    "                'address': address_data.get('raw', ''),\n",
    "                'created_at': vacancy.get('created_at'),\n",
    "                'published_at': vacancy.get('published_at'),\n",
    "                'requirement': snippet_data.get('requirement'),\n",
    "                'responsibility': snippet_data.get('responsibility'),\n",
    "                'experience': experience_data.get('name', ''),\n",
    "                'work_format': work_format_str,\n",
    "                'internship': vacancy.get('internship', False),\n",
    "                'premium': vacancy.get('premium', False),\n",
    "                'professional_role_ids': ','.join(role_ids),\n",
    "                'professional_role_names': ','.join(role_names),\n",
    "                'search_text': vacancy.get('search_text', '')\n",
    "            \n",
    "            }\n",
    "            processed_vacancies.append(processed_vacancy)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing vacancy {vacancy.get('id', 'Unknown ID')}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not processed_vacancies:\n",
    "        raise Exception(\"Failed to process any vacancies\")\n",
    "        \n",
    "    df = pd.DataFrame(processed_vacancies)\n",
    "    \n",
    "    # Fill empty values\n",
    "    df['salary_from'] = df['salary_from'].fillna(0)\n",
    "    df['salary_to'] = df['salary_to'].fillna(0)\n",
    "    df['salary_currency'] = df['salary_currency'].fillna('RUR')\n",
    "    df['requirement'] = df['requirement'].fillna('')\n",
    "    df['responsibility'] = df['responsibility'].fillna('')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:01:03.697328Z",
     "start_time": "2025-02-16T20:00:55.900702Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    api_client = create_api_client()\n",
    "    search_text_en = 'Product designer|Продуктовый дизайнер|TeamLead|Product manager|Product owner|CPO|Chief Product Officer|Golang Developer|Golang разработчик|PHP developer|Php разработчик|Product manager|Тестировщик|QA'\n",
    "    vacancies = get_all_vacancies(api_client, search_text_en, max_pages=5000)\n",
    "    df = create_vacancies_dataframe(vacancies)\n",
    "    \n",
    "    # Count records before removing duplicates\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Remove duplicates by id field, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset=['id'])\n",
    "    \n",
    "    # Count records after removing duplicates\n",
    "    final_count = len(df)\n",
    "    duplicates_removed = initial_count - final_count\n",
    "    print(f\"Количество удаленных дубликатов: {duplicates_removed}\")\n",
    "    \n",
    "    # Reset DataFrame index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Всего получено вакансий: {len(df)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка: {str(e)}\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get details of a single vacancy with retry\n",
    "def get_vacancy_details_with_retry(api_client, vacancy_id, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return get_vacancy_details(api_client, vacancy_id)\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed to get details for vacancy {vacancy_id} after {max_retries} attempts: {e}\")\n",
    "                return None\n",
    "            time.sleep(1)  # Small delay before retry\n",
    "\n",
    "# Function for parallel processing of vacancies\n",
    "def process_vacancy_details(vacancy_id):\n",
    "    try:\n",
    "        details = get_vacancy_details_with_retry(api_client, vacancy_id)\n",
    "        if details:\n",
    "            return {\n",
    "                'id': vacancy_id,  # Add vacancy ID to the result\n",
    "                'description': details.get('description', ''),\n",
    "                'key_skills': [skill.get('name') for skill in details.get('key_skills', [])]\n",
    "            }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing vacancy {vacancy_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Getting detailed information about vacancies...\")\n",
    "\n",
    "# Get list of all vacancy IDs\n",
    "vacancy_ids = df['id'].tolist()\n",
    "total_vacancies = len(vacancy_ids)\n",
    "completed_vacancies = 0\n",
    "\n",
    "# Create progress bar\n",
    "progress_bar = tqdm(total=total_vacancies, desc=\"Processing vacancies\")\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel processing\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Start getting details for all vacancies\n",
    "    future_to_id = {executor.submit(process_vacancy_details, vid): vid for vid in vacancy_ids}\n",
    "    \n",
    "    # Collect results\n",
    "    details_list = []\n",
    "    for future in as_completed(future_to_id):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            details_list.append(result)\n",
    "        # Update progress bar\n",
    "        completed_vacancies += 1\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# Close progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Create DataFrame with details\n",
    "details_df = pd.DataFrame(details_list)\n",
    "\n",
    "# Merge with main DataFrame using vacancy ID\n",
    "df = df.merge(details_df, on='id', how='left')\n",
    "df.info()\n",
    "# Fill empty values\n",
    "df['description'] = df['description'].fillna('')\n",
    "df['key_skills'] = df['key_skills'].fillna('').apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "print(f\"Details retrieved for {len(details_list)} out of {total_vacancies} vacancies\")\n",
    "print(\"\\nUpdated DataFrame structure:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"unique search_text: {df['search_text'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_one_vacancy = True\n",
    "if debug_one_vacancy:   \n",
    "    id = 103720577\n",
    "    details = get_vacancy_details_with_retry(api_client, id)\n",
    "    filename = f\"vacancy_detail_{id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(details, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_employer_details = True\n",
    "if debug_employer_details:\n",
    "    employer_id = 3529\n",
    "    employer_details = get_employer_details(api_client, employer_id)\n",
    "    filename = f\"employer_detail_{employer_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(employer_details, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:01:14.196192Z",
     "start_time": "2025-02-16T20:01:14.179942Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:06:39.454366Z",
     "start_time": "2025-02-16T20:06:39.445619Z"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('company_name').size().reset_index(name='count').sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T20:06:43.174468Z",
     "start_time": "2025-02-16T20:06:43.145540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Формируем имя файла\n",
    "filename = f\"vacancies_all_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "filepath = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "# Сохраняем DataFrame\n",
    "df.to_csv(filepath, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_bulk_dataframe(client, df, table_name):\n",
    "    # Convert DataFrame to a list of tuples and perform bulk insertion in a single query\n",
    "    data = list(df.itertuples(index=False, name=None))\n",
    "    columns = \", \".join(df.columns)\n",
    "    client.execute(f\"INSERT INTO {table_name} ({columns}) VALUES\", data)\n",
    "\n",
    "# Load configuration using the existing load_config function\n",
    "config = load_config()\n",
    "\n",
    "# Configure ClickHouse client using data from configuration\n",
    "client = Client(\n",
    "    host=config['clickhouse_host'],\n",
    "    port=9440,\n",
    "    user='default',\n",
    "    password=config['clickhouse_password'],\n",
    "    database=config.get('clickhouse_database', 'default'),\n",
    "    secure=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_for_clickhouse(df):\n",
    "    \"\"\"Преобразует DataFrame для соответствия схеме таблицы в ClickHouse\"\"\"\n",
    "    df_prepared = df.copy()\n",
    "    \n",
    "    # Заполняем пустые значения перед преобразованием типов\n",
    "    df_prepared['id'] = df_prepared['id'].fillna(0).astype(int)\n",
    "    df_prepared['company_id'] = df_prepared['company_id'].fillna(0).astype(int)\n",
    "    df_prepared['professional_role_ids'] = df_prepared['professional_role_ids'].fillna(0).astype(int)\n",
    "    \n",
    "    # Преобразуем поля с плавающей точкой\n",
    "    df_prepared['salary_from'] = pd.to_numeric(df_prepared['salary_from'], errors='coerce')\n",
    "    df_prepared['salary_to'] = pd.to_numeric(df_prepared['salary_to'], errors='coerce')\n",
    "    \n",
    "    # Преобразуем дату в формат DateTime\n",
    "    df_prepared['created_at'] = pd.to_datetime(df_prepared['created_at'])\n",
    "\n",
    "    df_prepared['published_at'] = pd.to_datetime(df_prepared['published_at'])\n",
    "    \n",
    "    # Заполняем None значения в строковых полях пустыми строками\n",
    "    for column in df_prepared.select_dtypes(include=['object']).columns:\n",
    "        df_prepared[column] = df_prepared[column].fillna('')\n",
    "    \n",
    "    # Преобразуем булевы значения в строки\n",
    "    bool_columns = df_prepared.select_dtypes(include=['bool']).columns\n",
    "    for column in bool_columns:\n",
    "        df_prepared[column] = df_prepared[column].astype(str)\n",
    "    \n",
    "    return df_prepared\n",
    "\n",
    "# Подготавливаем данные и вставляем в ClickHouse\n",
    "df_prepared = prepare_dataframe_for_clickhouse(df)\n",
    "insert_bulk_dataframe(client, df_prepared, 'vacancies_hh_ru')\n",
    "print(\"Data successfully inserted into ClickHouse.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
