{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from tqdm import tqdm \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c34a561de4e8a62c",
   "metadata": {},
   "source": [
    "# WebDriver configuration with automatic ChromeDriver installation\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run without opening the browser window\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aba9f2b00dcd58d9",
   "metadata": {},
   "source": [
    "# 🔧 Config\n",
    "SAVE_HTML = False  # Flag to control saving of HTML pages\n",
    "WAIT_TIME = 0.5   # Timeout in seconds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5af2d1f93d7658fa",
   "metadata": {},
   "source": [
    "# 📥 Save HTML for analysis (if flag is enabled)\n",
    "def save_html(page_source, page):\n",
    "    if SAVE_HTML:\n",
    "        folder_path = os.path.expanduser(\"~/pet-projects/jupyter-notebooks/data/hh_pages\")\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(folder_path, f\"page_{page}.html\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(page_source)\n",
    "        print(f\"✅ HTML of page {page + 1} saved to {file_path}\")\n",
    "\n",
    "# 🔄 Function to scroll through the page to load all vacancies\n",
    "def scroll_to_load_all_vacancies(driver):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_attempts = 0\n",
    "\n",
    "    while scroll_attempts < 10:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(WAIT_TIME)\n",
    "\n",
    "        # Wait for new vacancies to appear (DOM update)\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-qa^=\"vacancy-serp__vacancy\"]'))\n",
    "            )\n",
    "        except:\n",
    "            pass  # Proceed if no new elements are found\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            scroll_attempts += 1\n",
    "        else:\n",
    "            scroll_attempts = 0  # Reset if new elements appeared\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "    print(f\"📜 Scrolling completed. Page height: {last_height}\")\n",
    "\n",
    "# 📊 Get vacancies from one search result page\n",
    "def get_vacancy_links_and_companies(driver, keyword, page, vacancies_per_page):\n",
    "    url = f\"https://hh.ru/search/vacancy?text={keyword}&search_field=name&excluded_text=&salary=&currency_code=RUR&experience=doesNotMatter&order_by=relevance&search_period=0&items_on_page={vacancies_per_page}&L_save_area=true&page={page}\"\n",
    "    print(f\"\\n🔗 Loading page {page + 1}: {url}\")\n",
    "    \n",
    "    driver.get(url)\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-qa^=\"vacancy-serp__vacancy\"]'))\n",
    "        )\n",
    "    except:\n",
    "        print(f\"❗ Vacancies not loaded on page {page + 1}\")\n",
    "    scroll_to_load_all_vacancies(driver)\n",
    "    save_html(driver.page_source, page)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    results = []\n",
    "\n",
    "    vacancy_blocks = soup.select('div[data-qa^=\"vacancy-serp__vacancy\"]')\n",
    "    print(f\"📦 Found {len(vacancy_blocks)} vacancy blocks on page {page + 1}\")\n",
    "\n",
    "    for vacancy in vacancy_blocks:\n",
    "        link_tag = vacancy.select_one('a[data-qa=\"serp-item__title\"]')\n",
    "        company_tag = vacancy.find('a', {'data-qa': 'vacancy-serp__vacancy-employer'})\n",
    "        title_tag = vacancy.find('span', {'data-qa': 'serp-item__title-text'})\n",
    "        address_tag = vacancy.find('span', {'data-qa': 'vacancy-serp__vacancy-address'})\n",
    "\n",
    "        link = link_tag['href'] if link_tag else \"\"\n",
    "        if link and not link.startswith('http'):\n",
    "            link = f\"https://hh.ru{link}\"\n",
    "\n",
    "        company = company_tag.get_text(strip=True) if company_tag else \"Not specified\"\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"Title not specified\"\n",
    "        address = address_tag.get_text(strip=True) if address_tag else \"Address not specified\"\n",
    "\n",
    "        # Skip links containing \"adsrv.hh.ru\"\n",
    "        if \"adsrv.hh.ru\" not in link:\n",
    "            results.append({\n",
    "                \"Link\": link,\n",
    "                \"Company\": company,\n",
    "                \"Job Title\": title,\n",
    "                \"Address\": address\n",
    "            })\n",
    "\n",
    "    return results\n",
    "def extract_salary_info(salary_span):\n",
    "    \"\"\"\n",
    "    Извлекает информацию о зарплате из span элемента.\n",
    "    Возвращает строку вида \"start-end currency\" или \"amount currency\"\n",
    "    \"\"\"\n",
    "    if not salary_span:\n",
    "        return \"неизвестно\"\n",
    "    \n",
    "    try:\n",
    "        # Получаем весь текст из span\n",
    "        raw_salary = salary_span.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Находим все числа в строке\n",
    "        numbers = re.findall(r'\\d+(?:\\s*\\d+)*', raw_salary)\n",
    "        numbers = [n.replace(' ', '') for n in numbers]  # Убираем пробелы внутри чисел\n",
    "        \n",
    "        # Находим валюту (₽, $, €, etc.)\n",
    "        currency = re.search(r'[$₽€]', raw_salary)\n",
    "        currency = currency.group(0) if currency else '₽'  # По умолчанию рубли\n",
    "        \n",
    "        if len(numbers) == 2:\n",
    "            # Если найдено два числа - это диапазон\n",
    "            return f\"{numbers[0]}-{numbers[1]} {currency}\"\n",
    "        elif len(numbers) == 1:\n",
    "            # Если найдено одно число\n",
    "            return f\"{numbers[0]} {currency}\"\n",
    "        else:\n",
    "            return \"неизвестно\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ошибка при обработке зарплаты: {e}\")\n",
    "        return \"неизвестно\"\n",
    "        \n",
    "# ⚡ Function to obtain vacancy details (job description and salary info) using a separate driver instance\n",
    "def get_vacancy_details(link, index, total):\n",
    "    print(f\"➡️  [{index}/{total}] Loading details: {link}\")\n",
    "    try:\n",
    "        # Create a new driver instance for each vacancy detail extraction\n",
    "        driver_detail = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver_detail.get(link)\n",
    "        time.sleep(WAIT_TIME)\n",
    "        soup = BeautifulSoup(driver_detail.page_source, 'html.parser')\n",
    "        \n",
    "        # Extract job description from possible blocks\n",
    "        description = None\n",
    "        description_classes = ['vacancy-branded-user-content', 'vacancy-description']\n",
    "        for cls in description_classes:\n",
    "            description_tag = soup.find('div', class_=cls) or soup.find('div', {'data-qa': cls})\n",
    "            if description_tag:\n",
    "                description = description_tag.get_text(separator=' ', strip=True)\n",
    "                break\n",
    "        if description is None:\n",
    "            description = \"Description not available\"\n",
    "        \n",
    "        # Extract salary information\n",
    "        salary_info = \"неизвестно\"\n",
    "        salary_div = soup.find('div', {'data-qa': 'vacancy-salary'})\n",
    "        if salary_div:\n",
    "            salary_span = salary_div.find('span', {'data-qa': 'vacancy-salary-compensation-type-net'})\n",
    "            salary_info = extract_salary_info(salary_span)\n",
    "\n",
    "        driver_detail.quit()\n",
    "        return description, salary_info\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error on {link}: {e}\")\n",
    "        return \"Description not available\", \"неизвестно\"\n",
    "\n",
    "def get_vacancy_details_with_retry(link, index, total, max_attempts=5):\n",
    "    \"\"\"\n",
    "    Обёртка для get_vacancy_details с повтором при возникновении исключений.\n",
    "    Попытка повторяется до max_attempts раз.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        try:\n",
    "            result = get_vacancy_details(link, index, total)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            print(f\"❌ Ошибка при получении данных для {link} (попытка {attempt}/{max_attempts}): {e}\")\n",
    "            time.sleep(WAIT_TIME)\n",
    "    print(f\"⚠️ Для {link} превышено количество попыток ({max_attempts}). Возвращаем значения по умолчанию.\")\n",
    "    return (\"Description not available\", \"неизвестно\")\n",
    "\n",
    "\n",
    "# 🔍 Main scraping function that gathers vacancy data and then obtains detailed descriptions and salary info in parallel\n",
    "def scrape_hh_vacancy_descriptions(keyword):\n",
    "    vacancies_per_page = 100\n",
    "    driver.get(f\"https://hh.ru/search/vacancy?text={keyword}&search_field=name&excluded_text=&salary=&currency_code=RUR&experience=doesNotMatter&order_by=relevance&search_period=0&items_on_page={vacancies_per_page}&L_save_area=true\")\n",
    "    scroll_to_load_all_vacancies(driver)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    total_vacancies_tag = soup.select_one('div[data-qa=\"vacancies-search-header\"] h1[data-qa=\"title\"]')\n",
    "    total_vacancies = int(''.join(filter(str.isdigit, total_vacancies_tag.get_text(strip=True)))) if total_vacancies_tag else 0\n",
    "\n",
    "    print(f\"\\n🔍 Total vacancies found: {total_vacancies}\")\n",
    "    total_pages = (total_vacancies // vacancies_per_page) + (1 if total_vacancies % vacancies_per_page > 0 else 0)\n",
    "    print(f\"📄 Total pages: {total_pages}\")\n",
    "\n",
    "    all_vacancies = []\n",
    "    for page in range(total_pages):\n",
    "        vacancies = get_vacancy_links_and_companies(driver, keyword, page, vacancies_per_page)\n",
    "        all_vacancies.extend(vacancies)\n",
    "        time.sleep(WAIT_TIME)\n",
    "\n",
    "    print(f\"\\n📦 Total vacancies to process: {len(all_vacancies)}\")\n",
    "\n",
    "    # Parallel extraction of vacancy details\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_vacancy = {\n",
    "            executor.submit(get_vacancy_details_with_retry, vacancy['Link'], i+1, len(all_vacancies)): vacancy \n",
    "            for i, vacancy in enumerate(all_vacancies)\n",
    "        }\n",
    "        for future in tqdm(as_completed(future_to_vacancy), total=len(future_to_vacancy), desc=\"📥 Loading vacancy details\", unit=\"vacancy\"):\n",
    "            description, salary_info = future.result()\n",
    "            vacancy = future_to_vacancy[future]\n",
    "            vacancy['Job Description'] = description\n",
    "            vacancy['Salary Info'] = salary_info\n",
    "            time.sleep(WAIT_TIME)\n",
    "\n",
    "    df = pd.DataFrame(all_vacancies)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ea00c52",
   "metadata": {},
   "source": [
    "vacancy_name = \"TeamLead\"\n",
    "df_vacancies = scrape_hh_vacancy_descriptions(vacancy_name)\n",
    "df_vacancies.info()\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e92da614604e385",
   "metadata": {},
   "source": [
    "df_vacancies[\n",
    "    (df_vacancies['Address'] == 'Новосибирск')\n",
    "    # (df_vacancies['Job Title'].str.contains(\"VK\", na=False))\n",
    "].head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46de1ddf3461dad8",
   "metadata": {},
   "source": [
    "formatted_vacancy_name = vacancy_name.lower().replace(\" \", \"_\")\n",
    "file_path = os.path.expanduser(f\"~/pet-projects/jupyter-notebooks/vacancies_{formatted_vacancy_name}.csv\")\n",
    "df_vacancies.to_csv(file_path, index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ebeec8b1f90af6f",
   "metadata": {},
   "source": [
    "print(extract_salary_info('от <!-- -->400&nbsp;000<!-- --> <!-- -->₽<!-- --> за&nbsp;месяц<!-- -->, <span class=\"vacancy-salary-compensation-type\"> <!-- -->на руки'))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
