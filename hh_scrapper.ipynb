{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from tqdm import tqdm \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# WebDriver configuration with automatic ChromeDriver installation\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run without opening the browser window\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)"
   ],
   "id": "c34a561de4e8a62c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ðŸ”§ Config\n",
    "SAVE_HTML = True  # Flag to control saving of HTML pages\n",
    "WAIT_TIME = 0.5   # Timeout in seconds"
   ],
   "id": "aba9f2b00dcd58d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5af2d1f93d7658fa",
   "metadata": {},
   "source": [
    "# ðŸ“¥ Save HTML for analysis (if flag is enabled)\n",
    "def save_html(page_source, page):\n",
    "    if SAVE_HTML:\n",
    "        folder_path = os.path.expanduser(\"~/pet-projects/jupyter-notebooks/data/hh_pages\")\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(folder_path, f\"page_{page}.html\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(page_source)\n",
    "        print(f\"âœ… HTML of page {page + 1} saved to {file_path}\")\n",
    "\n",
    "# ðŸ”„ Scroll through the page to load all vacancies\n",
    "def scroll_to_load_all_vacancies(driver):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_attempts = 0\n",
    "\n",
    "    while scroll_attempts < 10:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(WAIT_TIME)\n",
    "\n",
    "        # Wait for new vacancies to appear (DOM update)\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-qa^=\"vacancy-serp__vacancy\"]'))\n",
    "            )\n",
    "        except:\n",
    "            pass  # Continue if no new elements are found\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            scroll_attempts += 1\n",
    "        else:\n",
    "            scroll_attempts = 0  # Reset attempts if new elements appeared\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "    print(f\"ðŸ“œ Scrolling completed. Page height: {last_height}\")\n",
    "\n",
    "# ðŸ“Š Get vacancies from one page\n",
    "def get_vacancy_links_and_companies(driver, keyword, page):\n",
    "    url = f\"https://hh.ru/search/vacancy?text={keyword}&search_field=name&excluded_text=&salary=&currency_code=RUR&experience=doesNotMatter&order_by=relevance&search_period=0&items_on_page=50&L_save_area=true&page={page}\"\n",
    "    print(f\"\\nðŸ”— Loading page {page + 1}: {url}\")\n",
    "    \n",
    "    driver.get(url)\n",
    "    # Wait until at least one vacancy appears on the page\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-qa^=\"vacancy-serp__vacancy\"]'))\n",
    "        )\n",
    "    except:\n",
    "        print(f\"â— Vacancies not loaded on page {page + 1}\")\n",
    "    scroll_to_load_all_vacancies(driver)\n",
    "    save_html(driver.page_source, page)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    results = []\n",
    "\n",
    "    vacancy_blocks = soup.select('div[data-qa^=\"vacancy-serp__vacancy\"]')\n",
    "    print(f\"ðŸ“¦ Found {len(vacancy_blocks)} vacancy blocks on page {page + 1}\")\n",
    "\n",
    "    for vacancy in vacancy_blocks:\n",
    "        link_tag = vacancy.select_one('a[data-qa=\"serp-item__title\"]')\n",
    "        company_tag = vacancy.find('a', {'data-qa': 'vacancy-serp__vacancy-employer'})\n",
    "        title_tag = vacancy.find('span', {'data-qa': 'serp-item__title-text'})  # Job title\n",
    "        address_tag = vacancy.find('span', {'data-qa': 'vacancy-serp__vacancy-address'})  # Address\n",
    "\n",
    "        link = link_tag['href'] if link_tag else \"\"\n",
    "        if link and not link.startswith('http'):\n",
    "            link = f\"https://hh.ru{link}\"\n",
    "\n",
    "        company = company_tag.get_text(strip=True) if company_tag else \"Not specified\"\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"Title not specified\"\n",
    "        address = address_tag.get_text(strip=True) if address_tag else \"Address not specified\"\n",
    "\n",
    "        if \"adsrv.hh.ru\" not in link:\n",
    "            results.append({\n",
    "                \"Link\": link,\n",
    "                \"Company\": company,\n",
    "                \"Job Title\": title,\n",
    "                \"Address\": address\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# ðŸ“„ Get job description\n",
    "def get_vacancy_description(driver, link, index, total):\n",
    "    print(f\"âž¡ï¸  [{index}/{total}] Loading description: {link}\")\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        time.sleep(WAIT_TIME)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        description_classes = [\n",
    "            'g-user-content',\n",
    "            'vacancy-branded-user-content',\n",
    "            'vacancy-description',\n",
    "        ]\n",
    "        for cls in description_classes:\n",
    "            description_tag = soup.find('div', class_=cls) or soup.find('div', {'data-qa': cls})\n",
    "            if description_tag:\n",
    "                return description_tag.get_text(separator=' ', strip=True)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading description for {link}: {e}\")\n",
    "    \n",
    "    return \"Description not available\"\n",
    "\n",
    "# ðŸ” Main function to scrape data\n",
    "def scrape_hh_vacancy_descriptions(keyword):\n",
    "    vacancies_per_page = 50\n",
    "    driver.get(f\"https://hh.ru/search/vacancy?text={keyword}&search_field=name&excluded_text=&salary=&currency_code=RUR&experience=doesNotMatter&order_by=relevance&search_period=0&items_on_page={vacancies_per_page}&L_save_area=true\")\n",
    "    scroll_to_load_all_vacancies(driver)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    total_vacancies_tag = soup.select_one('div[data-qa=\"vacancies-search-header\"] h1[data-qa=\"title\"]')\n",
    "    total_vacancies = int(''.join(filter(str.isdigit, total_vacancies_tag.get_text(strip=True)))) if total_vacancies_tag else 0\n",
    "\n",
    "    print(f\"\\nðŸ” Total vacancies found: {total_vacancies}\")\n",
    "\n",
    "    total_pages = (total_vacancies // vacancies_per_page) + (1 if total_vacancies % vacancies_per_page > 0 else 0)\n",
    "    print(f\"ðŸ“„ Total pages: {total_pages}\")\n",
    "\n",
    "    all_vacancies = []\n",
    "    for page in range(total_pages):\n",
    "        vacancies = get_vacancy_links_and_companies(driver, keyword, page)\n",
    "        all_vacancies.extend(vacancies)\n",
    "        time.sleep(WAIT_TIME)\n",
    "\n",
    "    print(f\"\\nðŸ“¦ Total vacancies to process: {len(all_vacancies)}\")\n",
    "\n",
    "    # Progress bar for loading descriptions\n",
    "    for i, vacancy in enumerate(tqdm(all_vacancies, desc=\"ðŸ“¥ Loading descriptions\", unit=\"vacancy\")):\n",
    "        vacancy['Job Description'] = get_vacancy_description(driver, vacancy['Link'], i + 1, len(all_vacancies))\n",
    "        time.sleep(WAIT_TIME)\n",
    "\n",
    "    df = pd.DataFrame(all_vacancies)\n",
    "    return df\n",
    "\n",
    "# ðŸš€ Run\n",
    "df_vacancies = scrape_hh_vacancy_descriptions(\"CPO\")\n",
    "df_vacancies.info()\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e92da614604e385",
   "metadata": {},
   "source": "df_vacancies.head(100)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path = os.path.expanduser(\"~/pet-projects/jupyter-notebooks/vacancies_cpo.csv\")\n",
    "df_vacancies.to_csv(file_path, index=False)"
   ],
   "id": "46de1ddf3461dad8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
