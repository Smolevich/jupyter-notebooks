{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from tqdm import tqdm \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c34a561de4e8a62c",
   "metadata": {},
   "source": [
    "# WebDriver configuration with automatic ChromeDriver installation\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run without opening the browser window\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aba9f2b00dcd58d9",
   "metadata": {},
   "source": [
    "# üîß Config\n",
    "SAVE_HTML = False  # Flag to control saving of HTML pages\n",
    "WAIT_TIME = 0.5   # Timeout in seconds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5af2d1f93d7658fa",
   "metadata": {},
   "source": [
    "# üì• Save HTML for analysis (if flag is enabled)\n",
    "def save_html(page_source, page):\n",
    "    if SAVE_HTML:\n",
    "        folder_path = os.path.expanduser(\"~/pet-projects/jupyter-notebooks/data/hh_pages\")\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(folder_path, f\"page_{page}.html\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(page_source)\n",
    "        print(f\"‚úÖ HTML of page {page + 1} saved to {file_path}\")\n",
    "\n",
    "# üîÑ Function to scroll through the page to load all vacancies\n",
    "def scroll_to_load_all_vacancies(driver):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_attempts = 0\n",
    "\n",
    "    while scroll_attempts < 10:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(WAIT_TIME)\n",
    "\n",
    "        # Wait for new vacancies to appear (DOM update)\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-qa^=\"vacancy-serp__vacancy\"]'))\n",
    "            )\n",
    "        except:\n",
    "            pass  # Proceed if no new elements are found\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        if new_height == last_height:\n",
    "            scroll_attempts += 1\n",
    "        else:\n",
    "            scroll_attempts = 0  # Reset if new elements appeared\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "    print(f\"üìú Scrolling completed. Page height: {last_height}\")\n",
    "\n",
    "# üìä Get vacancies from one search result page\n",
    "def get_vacancy_links_and_companies(driver, keyword, page, vacancies_per_page):\n",
    "    url = f\"https://hh.ru/search/vacancy?text={keyword}&search_field=name&excluded_text=&salary=&currency_code=RUR&experience=doesNotMatter&order_by=relevance&search_period=0&items_on_page={vacancies_per_page}&L_save_area=true&page={page}\"\n",
    "    print(f\"\\nüîó Loading page {page + 1}: {url}\")\n",
    "    \n",
    "    driver.get(url)\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-qa^=\"vacancy-serp__vacancy\"]'))\n",
    "        )\n",
    "    except:\n",
    "        print(f\"‚ùó Vacancies not loaded on page {page + 1}\")\n",
    "    scroll_to_load_all_vacancies(driver)\n",
    "    save_html(driver.page_source, page)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    results = []\n",
    "\n",
    "    vacancy_blocks = soup.select('div[data-qa^=\"vacancy-serp__vacancy\"]')\n",
    "    print(f\"üì¶ Found {len(vacancy_blocks)} vacancy blocks on page {page + 1}\")\n",
    "\n",
    "    for vacancy in vacancy_blocks:\n",
    "        link_tag = vacancy.select_one('a[data-qa=\"serp-item__title\"]')\n",
    "        company_tag = vacancy.find('a', {'data-qa': 'vacancy-serp__vacancy-employer'})\n",
    "        title_tag = vacancy.find('span', {'data-qa': 'serp-item__title-text'})\n",
    "        address_tag = vacancy.find('span', {'data-qa': 'vacancy-serp__vacancy-address'})\n",
    "\n",
    "        link = link_tag['href'] if link_tag else \"\"\n",
    "        if link and not link.startswith('http'):\n",
    "            link = f\"https://hh.ru{link}\"\n",
    "\n",
    "        company = company_tag.get_text(strip=True) if company_tag else \"Not specified\"\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"Title not specified\"\n",
    "        address = address_tag.get_text(strip=True) if address_tag else \"Address not specified\"\n",
    "\n",
    "        # Skip links containing \"adsrv.hh.ru\"\n",
    "        if \"adsrv.hh.ru\" not in link:\n",
    "            results.append({\n",
    "                \"Link\": link,\n",
    "                \"Company\": company,\n",
    "                \"Job Title\": title,\n",
    "                \"Address\": address\n",
    "            })\n",
    "\n",
    "    return results\n",
    "def extract_salary_info(salary_span):\n",
    "    \"\"\"\n",
    "    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞—Ä–ø–ª–∞—Ç–µ –∏–∑ span —ç–ª–µ–º–µ–Ω—Ç–∞.\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –≤–∏–¥–∞ \"start-end currency\" –∏–ª–∏ \"amount currency\"\n",
    "    \"\"\"\n",
    "    if not salary_span:\n",
    "        return \"–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ\"\n",
    "    \n",
    "    try:\n",
    "        # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏–∑ span\n",
    "        raw_salary = salary_span.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —á–∏—Å–ª–∞ –≤ —Å—Ç—Ä–æ–∫–µ\n",
    "        numbers = re.findall(r'\\d+(?:\\s*\\d+)*', raw_salary)\n",
    "        numbers = [n.replace(' ', '') for n in numbers]  # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤–Ω—É—Ç—Ä–∏ —á–∏—Å–µ–ª\n",
    "        \n",
    "        # –ù–∞—Ö–æ–¥–∏–º –≤–∞–ª—é—Ç—É (‚ÇΩ, $, ‚Ç¨, etc.)\n",
    "        currency = re.search(r'[$‚ÇΩ‚Ç¨]', raw_salary)\n",
    "        currency = currency.group(0) if currency else '‚ÇΩ'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä—É–±–ª–∏\n",
    "        \n",
    "        if len(numbers) == 2:\n",
    "            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –¥–≤–∞ —á–∏—Å–ª–∞ - —ç—Ç–æ –¥–∏–∞–ø–∞–∑–æ–Ω\n",
    "            return f\"{numbers[0]}-{numbers[1]} {currency}\"\n",
    "        elif len(numbers) == 1:\n",
    "            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –æ–¥–Ω–æ —á–∏—Å–ª–æ\n",
    "            return f\"{numbers[0]} {currency}\"\n",
    "        else:\n",
    "            return \"–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞—Ä–ø–ª–∞—Ç—ã: {e}\")\n",
    "        return \"–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ\"\n",
    "        \n",
    "# ‚ö° Function to obtain vacancy details (job description and salary info) using a separate driver instance\n",
    "def get_vacancy_details(link, index, total):\n",
    "    print(f\"‚û°Ô∏è  [{index}/{total}] Loading details: {link}\")\n",
    "    try:\n",
    "        # Create a new driver instance for each vacancy detail extraction\n",
    "        driver_detail = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver_detail.get(link)\n",
    "        time.sleep(WAIT_TIME)\n",
    "        soup = BeautifulSoup(driver_detail.page_source, 'html.parser')\n",
    "        \n",
    "        # Extract job description from possible blocks\n",
    "        description = None\n",
    "        description_classes = ['vacancy-branded-user-content', 'vacancy-description']\n",
    "        for cls in description_classes:\n",
    "            description_tag = soup.find('div', class_=cls) or soup.find('div', {'data-qa': cls})\n",
    "            if description_tag:\n",
    "                description = description_tag.get_text(separator=' ', strip=True)\n",
    "                break\n",
    "        if description is None:\n",
    "            description = \"Description not available\"\n",
    "        \n",
    "        # Extract salary information\n",
    "        salary_info = \"–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ\"\n",
    "        salary_div = soup.find('div', {'data-qa': 'vacancy-salary'})\n",
    "        if salary_div:\n",
    "            salary_span = salary_div.find('span', {'data-qa': 'vacancy-salary-compensation-type-net'})\n",
    "            salary_info = extract_salary_info(salary_span)\n",
    "\n",
    "        driver_detail.quit()\n",
    "        return description, salary_info\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error on {link}: {e}\")\n",
    "        return \"Description not available\", \"–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ\"\n",
    "\n",
    "def get_vacancy_details_with_retry(link, index, total, max_attempts=5):\n",
    "    \"\"\"\n",
    "    –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è get_vacancy_details —Å –ø–æ–≤—Ç–æ—Ä–æ–º –ø—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π.\n",
    "    –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –¥–æ max_attempts —Ä–∞–∑.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < max_attempts:\n",
    "        try:\n",
    "            result = get_vacancy_details(link, index, total)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {link} (–ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_attempts}): {e}\")\n",
    "            time.sleep(WAIT_TIME)\n",
    "    print(f\"‚ö†Ô∏è –î–ª—è {link} –ø—Ä–µ–≤—ã—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ ({max_attempts}). –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.\")\n",
    "    return (\"Description not available\", \"–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ\")\n",
    "\n",
    "\n",
    "# üîç Main scraping function that gathers vacancy data and then obtains detailed descriptions and salary info in parallel\n",
    "def scrape_hh_vacancy_descriptions(keyword):\n",
    "    vacancies_per_page = 100\n",
    "    driver.get(f\"https://hh.ru/search/vacancy?text={keyword}&search_field=name&excluded_text=&salary=&currency_code=RUR&experience=doesNotMatter&order_by=relevance&search_period=0&items_on_page={vacancies_per_page}&L_save_area=true\")\n",
    "    scroll_to_load_all_vacancies(driver)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    total_vacancies_tag = soup.select_one('div[data-qa=\"vacancies-search-header\"] h1[data-qa=\"title\"]')\n",
    "    total_vacancies = int(''.join(filter(str.isdigit, total_vacancies_tag.get_text(strip=True)))) if total_vacancies_tag else 0\n",
    "\n",
    "    print(f\"\\nüîç Total vacancies found: {total_vacancies}\")\n",
    "    total_pages = (total_vacancies // vacancies_per_page) + (1 if total_vacancies % vacancies_per_page > 0 else 0)\n",
    "    print(f\"üìÑ Total pages: {total_pages}\")\n",
    "\n",
    "    all_vacancies = []\n",
    "    for page in range(total_pages):\n",
    "        vacancies = get_vacancy_links_and_companies(driver, keyword, page, vacancies_per_page)\n",
    "        all_vacancies.extend(vacancies)\n",
    "        time.sleep(WAIT_TIME)\n",
    "\n",
    "    print(f\"\\nüì¶ Total vacancies to process: {len(all_vacancies)}\")\n",
    "\n",
    "    # Parallel extraction of vacancy details\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_vacancy = {\n",
    "            executor.submit(get_vacancy_details_with_retry, vacancy['Link'], i+1, len(all_vacancies)): vacancy \n",
    "            for i, vacancy in enumerate(all_vacancies)\n",
    "        }\n",
    "        for future in tqdm(as_completed(future_to_vacancy), total=len(future_to_vacancy), desc=\"üì• Loading vacancy details\", unit=\"vacancy\"):\n",
    "            description, salary_info = future.result()\n",
    "            vacancy = future_to_vacancy[future]\n",
    "            vacancy['Job Description'] = description\n",
    "            vacancy['Salary Info'] = salary_info\n",
    "            time.sleep(WAIT_TIME)\n",
    "\n",
    "    df = pd.DataFrame(all_vacancies)\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ea00c52",
   "metadata": {},
   "source": [
    "vacancy_name = \"TeamLead\"\n",
    "df_vacancies = scrape_hh_vacancy_descriptions(vacancy_name)\n",
    "df_vacancies.info()\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e92da614604e385",
   "metadata": {},
   "source": [
    "df_vacancies[\n",
    "    (df_vacancies['Address'] == '–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫')\n",
    "    # (df_vacancies['Job Title'].str.contains(\"VK\", na=False))\n",
    "].head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46de1ddf3461dad8",
   "metadata": {},
   "source": [
    "formatted_vacancy_name = vacancy_name.lower().replace(\" \", \"_\")\n",
    "file_path = os.path.expanduser(f\"~/pet-projects/jupyter-notebooks/vacancies_{formatted_vacancy_name}.csv\")\n",
    "df_vacancies.to_csv(file_path, index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ebeec8b1f90af6f",
   "metadata": {},
   "source": [
    "print(extract_salary_info('–æ—Ç <!-- -->400&nbsp;000<!-- --> <!-- -->‚ÇΩ<!-- --> –∑–∞&nbsp;–º–µ—Å—è—Ü<!-- -->, <span class=\"vacancy-salary-compensation-type\"> <!-- -->–Ω–∞ —Ä—É–∫–∏'))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
